{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lightweight-deputy",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gridworld import StochasticGridworld, DeterministicGridworld, make_random_gridworld\n",
    "from agents import RandomAgent, Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-yesterday",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = StochasticGridworld()\n",
    "random_agent = RandomAgent(task.observation_space, task.action_space)\n",
    "\n",
    "obs = task.reset()\n",
    "\n",
    "rewards = np.zeros((100, 100))\n",
    "for run in range(100):\n",
    "    for step in range(100):\n",
    "        act = random_agent(obs)\n",
    "        obs, rew, done, info = task.step(act)\n",
    "        rewards[run, step] = rew\n",
    "        \n",
    "print(\"Average return: {}\".format(rewards.sum(1).mean()))\n",
    "print(\"Standard deviation: {}\".format(rewards.sum(1).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-packing",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationAgent(Agent):\n",
    "    \n",
    "    def __init__(self, observation_space, action_space, gamma=0.9):   \n",
    "        self.actions = action_space\n",
    "        self.states = observation_space\n",
    "        \n",
    "        self.policy = np.zeros((self.states,), dtype=np.int)\n",
    "        self.values  = np.zeros((self.states,), dtype=np.int)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "    def policy_evaluation(self, transitions, rewards):\n",
    "        \"\"\"\n",
    "        transitions: np.array of size (S, A, S), with the corresponding to the current state, \n",
    "            the second to the action, and the third to the next state. The matrix encodes transition\n",
    "            probabilities\n",
    "        rewards: np.array of size (S) with single step rewards (reward are given for current state and are \n",
    "            independent of action)\n",
    "        \n",
    "        returns np.array of size(S) with new values after one step of policy evaluation\n",
    "        \"\"\"\n",
    "        ??? #TODO: Insert code here\n",
    "        \n",
    "    def policy_improvement(self, transitions, rewards):\n",
    "        \"\"\"\n",
    "        transitions: np.array of size (S, A, S), with the corresponding to the current state, \n",
    "            the second to the action, and the third to the next state. The matrix encodes transition\n",
    "            probabilities\n",
    "        rewards: np.array of size (S) with single step rewards (reward are given for current state and are \n",
    "            independent of action)\n",
    "        \n",
    "        returns np.array of size(S) with new actions after one step of policy improvement\n",
    "        \"\"\"\n",
    "        ??? #TODO: Insert code here\n",
    "    \n",
    "    def run_policy_iteration(self, transitions, rewards):\n",
    "        \"\"\"\n",
    "        Implements the main policy improvement loop. Do not change this code!\n",
    "        \"\"\"\n",
    "        done_policy = False\n",
    "        \n",
    "        while not done_policy:\n",
    "            done_value = False\n",
    "            \n",
    "            while not done_value:\n",
    "                last_values = self.values.copy()\n",
    "                self.values = self.policy_evaluation(transitions, rewards)\n",
    "                done_value = np.all(np.isclose(last_values, self.values))\n",
    "            last_policy = self.policy.copy()\n",
    "            self.policy = self.policy_improvement(transitions, rewards)\n",
    "            done_policy = np.all(last_policy == self.policy)\n",
    "            \n",
    "agent = PolicyIterationAgent(task.observation_space, task.action_space)\n",
    "agent.run_policy_iteration(task.state_transition, task.rewards)\n",
    "\n",
    "obs = task.reset()\n",
    "\n",
    "rewards = np.zeros((100, 100))\n",
    "for run in range(100):\n",
    "    for step in range(100):\n",
    "        act = agent(obs)\n",
    "        obs, rew, done, info = task.step(act)\n",
    "        rewards[run, step] = rew\n",
    "        \n",
    "print(\"Average return: {}\".format(rewards.sum(1).mean()))\n",
    "print(\"Standard deviation: {}\".format(rewards.sum(1).std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-heater",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "You can reuse code here where applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIterationAgent(Agent):\n",
    "    \n",
    "    def __init__(observation_space, action_space):   \n",
    "        self.actions = action_space\n",
    "        self.states = observation_space\n",
    "        \n",
    "        self.policy = np.zeros((self.states,), dtype=np.int)\n",
    "        self.values  = np.zeros((self.states,), dtype=np.int)\n",
    "\n",
    "    def policy_evaluation(self, transitions, rewards):\n",
    "        \"\"\"\n",
    "        transitions: np.array of size (S, A, S), with the corresponding to the current state, \n",
    "            the second to the action, and the third to the next state. The matrix encodes transition\n",
    "            probabilities\n",
    "        rewards: np.array of size (S) with single step rewards (reward are given for current state and are \n",
    "            independent of action)\n",
    "        \n",
    "        returns np.array of size(S) with new values after one step of policy evaluation\n",
    "        \"\"\"\n",
    "        ??? #TODO: Insert code here\n",
    "        \n",
    "    def policy_improvement(self):\n",
    "        \"\"\"\n",
    "        transitions: np.array of size (S, A, S), with the corresponding to the current state, \n",
    "            the second to the action, and the third to the next state. The matrix encodes transition\n",
    "            probabilities\n",
    "        rewards: np.array of size (S) with single step rewards (reward are given for current state and are \n",
    "            independent of action)\n",
    "        \n",
    "        returns np.array of size(S) with new actions after one step of policy improvement\n",
    "        \"\"\"\n",
    "        ??? #TODO: Insert code here\n",
    "        \n",
    "    def run_value_iteration(self, transitions, rewards):\n",
    "        ??? #TODO: Insert code here\n",
    "        \n",
    "agent = ValueIterationAgent(task.observation_space, task.action_space)\n",
    "agent.run_value_iteration(task.state_transition, task.rewards)\n",
    "\n",
    "obs = task.reset()\n",
    "\n",
    "rewards = np.zeros((100, 100))\n",
    "for run in range(100):\n",
    "    for step in range(100):\n",
    "        act = agent(obs)\n",
    "        obs, rew, done, info = task.step(act)\n",
    "        rewards[run, step] = rew\n",
    "        \n",
    "print(\"Average return: {}\".format(rewards.sum(1).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-cameroon",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_random_gridworld returns a randomized gridworld with \n",
    "# transitions and rewards sampled from a uniform and normal \n",
    "# distributions respectively\n",
    "\n",
    "# task = make_random_gridworld(num_states, num_actions)\n",
    "\n",
    "\n",
    "# create at least 100 gridworlds of different sizes and plot the runtime of your algorithm\n",
    "# you can use the python time module for timing\n",
    "\n",
    "# POLICY ITERATION\n",
    "pi_times = []\n",
    "\n",
    "num_states = ???\n",
    "num_actions = ???\n",
    "\n",
    "for state, action in zip(num_states, num_actions):\n",
    "    runtime = 0.\n",
    "    for _ in range(10):\n",
    "        ???\n",
    "        runtime += end - start\n",
    "    pi_times.append(runtime)\n",
    "    \n",
    "# VALUE ITERATION\n",
    "vi_times = []\n",
    "\n",
    "num_states = ???\n",
    "num_actions = ???\n",
    "\n",
    "for state, action in zip(num_states, num_actions):\n",
    "    runtime = 0.\n",
    "    for _ in range(10):\n",
    "        ???\n",
    "        runtime += end - start\n",
    "    pi_times.append(runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-tracker",
   "metadata": {},
   "source": [
    "Do the results conform with your expectations? Write a small discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-individual",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
