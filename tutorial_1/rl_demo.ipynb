{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "authorized-position",
   "metadata": {},
   "source": [
    "# RL Demonstration usinge stable baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.deepq.policies import MlpPolicy as DQNPolicy\n",
    "from stable_baselines.deepq.policies import CnnPolicy as DQNCnnPolicy\n",
    "from stable_baselines.common.policies import MlpPolicy as CommonPolicy\n",
    "from stable_baselines import DQN, PPO2, SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-recipe",
   "metadata": {},
   "source": [
    "Import all necessary libraries as you would in any other python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_plot(env_name, model, num_runs=32, run_length=500, render=False):\n",
    "    env = gym.make(env_name)\n",
    "    num_runs = 32\n",
    "    run_length = 500\n",
    "\n",
    "    all_rewards = np.zeros((run_length, num_runs))\n",
    "    for i in range(num_runs):\n",
    "        obs = env.reset()\n",
    "        for j in range(run_length):\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, rewards, dones, info = env.step(action)\n",
    "            all_rewards[j,i] = rewards\n",
    "            if render:\n",
    "                env.render()\n",
    "            if dones:\n",
    "                break\n",
    "        env.close()\n",
    "    print(all_rewards.sum(0))\n",
    "    plt.plot(range(run_length), np.mean(all_rewards, 1))\n",
    "    plt.fill_between(range(run_length), \n",
    "                     np.quantile(all_rewards, .25, 1), \n",
    "                     np.quantile(all_rewards, .75, 1), \n",
    "                     alpha=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-expert",
   "metadata": {},
   "source": [
    "Quick evaluation utility, details are explained below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-condition",
   "metadata": {},
   "source": [
    "Create an environment from the Gym examples. For most of the coding exercises, we will be using gym, since it is easy to install and has a very commonly used interface which you will find in many other simulators too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-springfield",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = DQN(DQNPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=500, log_interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-hotel",
   "metadata": {},
   "source": [
    "This is the main training loop. For the purpose of this demonstration, I used a preimplemented algorithm which hides all the complexity. In the class, coding these functions is of course your main goal ;)\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The following cells load some pretrained algorithms to show how a basic RL evaluation looks and what you can expect to see when testing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAC.load(\"models/sac_pendulum\")\n",
    "eval_and_plot(\"Pendulum-v0\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2.load(\"models/ppo_acrobot_success\")\n",
    "eval_and_plot(\"Acrobot-v1\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2.load(\"models/ppo_cartpole_success\")\n",
    "eval_and_plot(\"CartPole-v1\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(\"models/dqn_cartpole\")\n",
    "eval_and_plot(\"CartPole-v1\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_plot(env_name, model, num_runs=32, run_length=500, render=False):\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    all_rewards = np.zeros((run_length, num_runs))\n",
    "    for i in range(num_runs):\n",
    "        obs = env.reset()\n",
    "        for j in range(run_length):;)\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, rewards, dones, info = env.step(action)\n",
    "            all_rewards[j,i] = rewards\n",
    "            if render:\n",
    "                env.render()\n",
    "            if dones:\n",
    "                break\n",
    "        env.close()\n",
    "    print(all_rewards.sum(0))\n",
    "    plt.plot(range(run_length), np.mean(all_rewards, 1))\n",
    "    plt.fill_between(range(run_length), \n",
    "                     np.mean(all_rewards, 1) + np.std(all_rewards, 1), \n",
    "                     np.mean(all_rewards, 1) - np.std(all_rewards, 1), \n",
    "                     alpha=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-lying",
   "metadata": {},
   "source": [
    "## A closer look at the env code\n",
    "\n",
    "Below you can find a commented version of the evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "model = PPO2.load(\"models/ppo_cartpole_success\")\n",
    "num_runs = 32\n",
    "run_length = 200\n",
    "\n",
    "# env.reset initializes the environment and returns the first state observation\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the core loop, the environment is advanced one step at a time\n",
    "for _ in range(run_length):\n",
    "    # in case you need a random baseline, environments allow you to\n",
    "    # generate random actions directly from the action space\n",
    "    action = env.action_space.sample()\n",
    "    print(_)\n",
    "    \n",
    "    # the step() function takes the action as an argument and returns a tuple\n",
    "    # of the next observation, the direct reward, a boolean indicating whether\n",
    "    # a final state was reached, and a dictionary containing additional information\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "\n",
    "    # you can render the environment on your screen with this function\n",
    "    # note that the render function might have trouble drawing from a jupyter\n",
    "    # notebook\n",
    "    env.render()\n",
    "    \n",
    "    # a common pattern is to reset the environment once you have observed a \"done\"\n",
    "    # signal, this is not automatically done by the framework\n",
    "    if dones:\n",
    "        break\n",
    "\n",
    "# closing the env cleans up any initialized threads, especially renderings\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-guyana",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
